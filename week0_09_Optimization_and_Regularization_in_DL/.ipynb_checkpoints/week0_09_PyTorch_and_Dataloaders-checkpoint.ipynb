{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## week0_09 practice: PyTorch practice, hints and Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credits:\n",
    "* First part is based on YSDA [Practical RL course week04 materials](https://github.com/yandexdataschool/Practical_RL/tree/master/week04_%5Brecap%5D_deep_learning).\n",
    "* Second part is based on PyTorch official tutorials and [this kaggle kernel](https://www.kaggle.com/pinocookie/pytorch-dataset-and-dataloader)\n",
    "* Third part is based on PyTorch tutorial by [Stanford CS 231n course](http://cs231n.stanford.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://pytorch.org/tutorials/_static/pytorch-logo-dark.svg)\n",
    "\n",
    "__This notebook__ will remind you how to use pytorch low and high-level core. You can install it [here](http://pytorch.org/).\n",
    "\n",
    "__Pytorch feels__ differently than other frameworks (like tensorflow/theano) on almost every level. TensorFlow makes your code live in two \"worlds\" simultaneously:  symbolic graphs and actual tensors. First you declare a symbolic \"recipe\" of how to get from inputs to outputs, then feed it with actual minibatches of data.  In pytorch, __there's only one world__: all tensors have a numeric value.\n",
    "\n",
    "You compute outputs on the fly without pre-declaring anything. The code looks exactly as in pure numpy with one exception: pytorch computes gradients for you. And can run stuff on GPU. And has a number of pre-implemented building blocks for your neural nets. [And a few more things.](https://medium.com/towards-data-science/pytorch-vs-tensorflow-spotting-the-difference-25c75777377b)\n",
    "\n",
    "Let's dive into it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T00:59:42.103233Z",
     "start_time": "2020-02-28T00:59:42.099338Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import torchvision\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Tensormancy\n",
    "\n",
    "__1.1 The [_disclaimer_](https://gist.githubusercontent.com/justheuristic/e2c1fa28ca02670cabc42cacf3902796/raw/fd3d935cef63a01b85ed2790b5c11c370245cbd7/stddisclaimer.h)__\n",
    "\n",
    "Let's write another function, this time in polar coordinates:\n",
    "$$\\rho(\\theta) = (1 + 0.9 \\cdot cos (6 \\cdot \\theta) ) \\cdot (1 + 0.01 \\cdot cos(24 \\cdot \\theta)) \\cdot (0.5 + 0.05 \\cdot cos(200 \\cdot \\theta)) \\cdot (10 + sin(10 \\cdot \\theta))$$\n",
    "\n",
    "\n",
    "Then convert it into cartesian coordinates ([howto](http://www.mathsisfun.com/polar-cartesian-coordinates.html)) and plot the results.\n",
    "\n",
    "Use torch tensors only: no lists, loops, numpy arrays, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T00:13:20.890514Z",
     "start_time": "2020-02-28T00:13:20.639022Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-bd8980c812d7>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-bd8980c812d7>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    rho = ### YOUR CODE HERE\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "theta = torch.linspace(-np.pi, np.pi, steps=1000)\n",
    "\n",
    "# compute rho(theta) as per formula above\n",
    "rho = ### YOUR CODE HERE\n",
    "\n",
    "# Now convert polar (rho, theta) pairs into cartesian (x,y) to plot them.\n",
    "x = ### YOUR CODE HERE\n",
    "y = ### YOUR CODE HERE\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.fill(x.numpy(), y.numpy(), color='red')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Using the Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T00:13:20.896055Z",
     "start_time": "2020-02-28T00:13:20.893241Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T00:13:21.223097Z",
     "start_time": "2020-02-28T00:13:21.220643Z"
    }
   },
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/ml-mipt/ml-mipt/basic_s20/week0_09_Optimization_and_Regularization_in_DL/notmnist.py -nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T00:13:24.681345Z",
     "start_time": "2020-02-28T00:13:21.715775Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data...\n",
      "Extracting ...\n",
      "Parsing...\n",
      "found broken img: ./notMNIST_small/A/RGVtb2NyYXRpY2FCb2xkT2xkc3R5bGUgQm9sZC50dGY=.png [it's ok if <10 images are broken]\n",
      "found broken img: ./notMNIST_small/F/Q3Jvc3NvdmVyIEJvbGRPYmxpcXVlLnR0Zg==.png [it's ok if <10 images are broken]\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "from notmnist import load_notmnist\n",
    "X_train, y_train, X_test, y_test = load_notmnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T08:04:08.525398Z",
     "start_time": "2020-02-28T08:04:08.519240Z"
    }
   },
   "outputs": [],
   "source": [
    "class DatasetMNIST(Dataset):\n",
    "    def __init__(self, path='./notMNIST_small', letters='ABCDEFGHIJ', transform=None):\n",
    "        self.data, self.labels, _ ,_  = load_notmnist(path=path, letters=letters, test_size=0)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # load image as ndarray type (Height * Width * Channels)\n",
    "        # be carefull for converting dtype to np.uint8 [Unsigned integer (0 to 255)]\n",
    "        # in this example, i don't use ToTensor() method of torchvision.transforms\n",
    "        # so you can convert numpy ndarray shape to tensor in PyTorch (H, W, C) --> (C, H, W)\n",
    "        image = self.data[index].transpose(1, 2, 0)\n",
    "        label = self.labels[index]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T08:04:19.890914Z",
     "start_time": "2020-02-28T08:04:19.336026Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing...\n",
      "found broken img: ./notMNIST_small/A/RGVtb2NyYXRpY2FCb2xkT2xkc3R5bGUgQm9sZC50dGY=.png [it's ok if <10 images are broken]\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "full_dataset = DatasetMNIST('./notMNIST_small', 'AB', transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T08:05:36.271686Z",
     "start_time": "2020-02-28T08:05:36.267914Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 1)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# we can access and get data with index by __getitem__(index)\n",
    "img, lab = full_dataset.__getitem__(0)\n",
    "\n",
    "print(img.shape)\n",
    "print(type(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T08:05:39.406582Z",
     "start_time": "2020-02-28T08:05:39.402397Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torchvision.transforms.ToTensor()\n",
    "\n",
    "a(img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T00:13:37.378205Z",
     "start_time": "2020-02-28T00:13:37.115219Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "inds = np.random.randint(len(full_dataset), size=2)\n",
    "\n",
    "for i in range(2):\n",
    "    plt.subplot(1, 2, i + 1)\n",
    "    plt.imshow(full_dataset[inds[i]][0].reshape([28,28]))\n",
    "    plt.title(str(full_dataset[inds[i]][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To the DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T00:13:38.561437Z",
     "start_time": "2020-02-28T00:13:38.558371Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(full_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use dataloader as iterator by using iter() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T00:13:39.386457Z",
     "start_time": "2020-02-28T00:13:39.382386Z"
    }
   },
   "outputs": [],
   "source": [
    "train_iter = iter(train_loader)\n",
    "print(type(train_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at images and labels of batch size by extracting data `.next()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T00:13:40.508098Z",
     "start_time": "2020-02-28T00:13:40.502629Z"
    }
   },
   "outputs": [],
   "source": [
    "images, labels = train_iter.next()\n",
    "\n",
    "print('images shape on batch size = {}'.format(images.size()))\n",
    "print('labels shape on batch size = {}'.format(labels.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T00:13:41.593728Z",
     "start_time": "2020-02-28T00:13:41.506364Z"
    }
   },
   "outputs": [],
   "source": [
    "# make grid takes tensor as arg\n",
    "# tensor : (batchsize, channels, height, width)\n",
    "grid = torchvision.utils.make_grid(images.permute([0, 3, 1, 2]))\n",
    "\n",
    "plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
    "plt.axis('off')\n",
    "plt.title(labels.numpy());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now with transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T00:13:48.795066Z",
     "start_time": "2020-02-28T00:13:46.451771Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset_with_transform = DatasetMNIST(\n",
    "    transform=torchvision.transforms.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T00:13:48.801336Z",
     "start_time": "2020-02-28T00:13:48.797410Z"
    }
   },
   "outputs": [],
   "source": [
    "img, lab = train_dataset_with_transform.__getitem__(0)\n",
    "\n",
    "print('image shape at the first row : {}'.format(img.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T00:13:52.064233Z",
     "start_time": "2020-02-28T00:13:52.057623Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loader_tr = DataLoader(train_dataset_with_transform, batch_size=8, shuffle=True)\n",
    "\n",
    "train_iter_tr = iter(train_loader_tr)\n",
    "print(type(train_iter_tr))\n",
    "\n",
    "images, labels = train_iter_tr.next()\n",
    "\n",
    "print('images shape on batch size = {}'.format(images.size()))\n",
    "print('labels shape on batch size = {}'.format(labels.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T00:13:53.200621Z",
     "start_time": "2020-02-28T00:13:53.062965Z"
    }
   },
   "outputs": [],
   "source": [
    "grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
    "plt.axis('off')\n",
    "plt.title(labels.numpy());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composing several transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to take data augmentation, you have to make List using `torchvision.transforms.Compose`\n",
    "\n",
    "```\n",
    "class Compose(object):\n",
    "    \"\"\"Composes several transforms together.\n",
    "    Args:\n",
    "        transforms (list of ``Transform`` objects): list of transforms to compose.\n",
    "    Example:\n",
    "        >>> transforms.Compose([\n",
    "        >>>     transforms.CenterCrop(10),\n",
    "        >>>     transforms.ToTensor(),\n",
    "        >>> ])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img):\n",
    "        for t in self.transforms:\n",
    "            img = t(img)\n",
    "        return img\n",
    "\n",
    "    def __repr__(self):\n",
    "        format_string = self.__class__.__name__ + '('\n",
    "        for t in self.transforms:\n",
    "            format_string += '\\n'\n",
    "            format_string += '    {0}'.format(t)\n",
    "        format_string += '\\n)'\n",
    "        return format_string\n",
    "```\n",
    "\n",
    "\n",
    "this function can convert some image by order within `__call__` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T00:13:54.766219Z",
     "start_time": "2020-02-28T00:13:54.762711Z"
    }
   },
   "outputs": [],
   "source": [
    "class Flatten():\n",
    "    def __call__(self, pic):\n",
    "        return pic.flatten()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T00:13:55.430127Z",
     "start_time": "2020-02-28T00:13:55.427338Z"
    }
   },
   "outputs": [],
   "source": [
    "a = Flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T00:13:56.147330Z",
     "start_time": "2020-02-28T00:13:56.143060Z"
    }
   },
   "outputs": [],
   "source": [
    "a(img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T00:13:56.918185Z",
     "start_time": "2020-02-28T00:13:56.915283Z"
    }
   },
   "outputs": [],
   "source": [
    "new_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    Flatten()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T09:37:37.781950Z",
     "start_time": "2020-02-28T09:37:37.779388Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T00:56:29.075866Z",
     "start_time": "2020-02-28T00:56:29.071564Z"
    }
   },
   "outputs": [],
   "source": [
    "# use GPU if available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T09:28:18.459203Z",
     "start_time": "2020-02-28T09:28:18.455997Z"
    }
   },
   "outputs": [],
   "source": [
    "def subset_ind(dataset, ratio: float):\n",
    "#     return ### YOUR CODE HERE\n",
    "    return np.random.choice(len(dataset), size=int(ratio*len(dataset)), replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T09:32:54.001780Z",
     "start_time": "2020-02-28T09:32:51.635588Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = DatasetMNIST(\n",
    "    './notMNIST_small',\n",
    "#     'AB',\n",
    "    transform=new_transform\n",
    ")\n",
    "\n",
    "shrink_inds = subset_ind(dataset, 0.2)\n",
    "dataset = Subset(dataset, shrink_inds)\n",
    "\n",
    "print(f'\\n\\n dataset size: {len(dataset)}, labels: {np.unique(dataset.dataset.labels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T09:32:54.028315Z",
     "start_time": "2020-02-28T09:32:54.004282Z"
    }
   },
   "outputs": [],
   "source": [
    "val_size = 0.2\n",
    "val_inds = subset_ind(dataset, val_size)\n",
    "\n",
    "train_dataset = Subset(dataset, [i for i in range(len(dataset)) if i not in val_inds])\n",
    "val_dataset = Subset(dataset, val_inds)\n",
    "\n",
    "print(f'  training size: {len(train_dataset)}\\nvalidation size: {len(val_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T09:32:59.926520Z",
     "start_time": "2020-02-28T09:32:59.922784Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T09:33:00.954231Z",
     "start_time": "2020-02-28T09:33:00.947835Z"
    }
   },
   "outputs": [],
   "source": [
    "train_iter = iter(train_loader)\n",
    "print(type(train_iter))\n",
    "\n",
    "images, labels = train_iter.next()\n",
    "\n",
    "print('images shape on batch size = {}'.format(images.size()))\n",
    "print('labels shape on batch size = {}'.format(labels.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T09:35:42.679368Z",
     "start_time": "2020-02-28T09:35:42.676594Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T09:35:44.204284Z",
     "start_time": "2020-02-28T09:35:44.199944Z"
    }
   },
   "outputs": [],
   "source": [
    "# create network again just in case\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(784, 10),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "model.to(device, torch.float32)\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T09:33:02.897035Z",
     "start_time": "2020-02-28T09:33:02.884404Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, loss_fn, opt, n_epochs: int):\n",
    "    '''\n",
    "    model: нейросеть для обучения,\n",
    "    train_loader, val_loader: загрузчики данных\n",
    "    loss_fn: целевая метрика (которую будем оптимизировать)\n",
    "    opt: оптимизатор (обновляет веса нейросети)\n",
    "    n_epochs: кол-во эпох, полных проходов датасета\n",
    "    '''\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    val_accuracy = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        ep_train_loss = []\n",
    "        ep_val_loss = []\n",
    "        ep_val_accuracy = []\n",
    "        start_time = time.time()\n",
    "\n",
    "        model.train(True) # enable dropout / batch_norm training behavior\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            # move data to target device\n",
    "            ### YOUR CODE HERE\n",
    "\n",
    "            # train on batch: compute loss, calc grads, perform optimizer step and zero the grads\n",
    "            ### YOUR CODE HERE\n",
    "            ep_train_loss.append(loss.item())\n",
    "\n",
    "        model.train(False) # disable dropout / use averages for batch_norm\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                # move data to target device\n",
    "                ### YOUR CODE HERE\n",
    "\n",
    "                # compute predictions\n",
    "                ### YOUR CODE HERE\n",
    "                ep_val_loss.append(### YOUR CODE HERE)\n",
    "                y_pred = ### YOUR CODE HERE\n",
    "                ep_val_accuracy.append(### YOUR CODE HERE)\n",
    "\n",
    "        # print the results for this epoch:\n",
    "        print(f'Epoch {epoch + 1} of {n_epochs} took {time.time() - start_time:.3f}s')\n",
    "\n",
    "        train_loss.append(np.mean(ep_train_loss))\n",
    "        val_loss.append(np.mean(ep_val_loss))\n",
    "        val_accuracy.append(np.mean(ep_val_accuracy))\n",
    "        \n",
    "        print(f\"\\t  training loss: {train_loss[-1]:.6f}\")\n",
    "        print(f\"\\tvalidation loss: {val_loss[-1]:.6f}\")\n",
    "        print(f\"\\tvalidation accuracy: {val_accuracy[-1]:.3f}\")\n",
    "\n",
    "    return train_loss, val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T09:33:33.252494Z",
     "start_time": "2020-02-28T09:33:28.712014Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 30\n",
    "\n",
    "train_loss, val_loss, val_accuracy = train_model(model, train_loader, val_loader, loss_func, opt, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T09:33:35.242213Z",
     "start_time": "2020-02-28T09:33:35.237045Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_train_process(train_loss, val_loss, val_accuracy):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    axes[0].set_title('Loss')\n",
    "    axes[0].plot(train_loss, label='train')\n",
    "    axes[0].plot(val_loss, label='validation')\n",
    "    axes[0].legend()\n",
    "\n",
    "    axes[1].set_title('Validation accuracy')\n",
    "    axes[1].plot(val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T09:33:36.057391Z",
     "start_time": "2020-02-28T09:33:35.611228Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_train_process(train_loss, val_loss, val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T09:44:50.420339Z",
     "start_time": "2020-02-28T09:44:50.406364Z"
    }
   },
   "outputs": [],
   "source": [
    "# create network again just in case\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(784, 500),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(500, 200),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(200, 10),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "model.to(device, torch.float32)\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T09:45:08.145156Z",
     "start_time": "2020-02-28T09:44:52.924801Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 30\n",
    "\n",
    "train_loss, val_loss, val_accuracy = train_model(model, train_loader, val_loader, loss_func, opt, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T09:45:13.396284Z",
     "start_time": "2020-02-28T09:45:13.032994Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_train_process(train_loss, val_loss, val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your turn\n",
    "Try to add some additional transformations (e.g. random crop, rotation etc.) and train your model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batchnorm try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Save the model (model checkpointing)\n",
    "\n",
    "Now we have trained a model! Obviously we do not want to retrain the model everytime we want to use it. Plus if you are training a super big model, you probably want to save checkpoint periodically so that you can always fall back to the last checkpoint in case something bad happened or you simply want to test models at different training iterations.\n",
    "\n",
    "Model checkpointing is fairly simple in PyTorch. First, we define a helper function that can save a model to the disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(checkpoint_path, model, optimizer):\n",
    "    # state_dict: a Python dictionary object that:\n",
    "    # - for a model, maps each layer to its parameter tensor;\n",
    "    # - for an optimizer, contains info about the optimizer’s states and hyperparameters used.\n",
    "    state = {\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer' : optimizer.state_dict()}\n",
    "    torch.save(state, checkpoint_path)\n",
    "    print('model saved to %s' % checkpoint_path)\n",
    "    \n",
    "def load_checkpoint(checkpoint_path, model, optimizer):\n",
    "    state = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(state['state_dict'])\n",
    "    optimizer.load_state_dict(state['optimizer'])\n",
    "    print('model loaded from %s' % checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a brand new model\n",
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "# Testing -- you should get a pretty poor performance since the model hasn't learned anything yet.\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a training loop with model checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_save(epoch, save_interval, log_interval=100):\n",
    "    model.train()  # set training mode\n",
    "    iteration = 0\n",
    "    for ep in range(epoch):\n",
    "        for batch_idx, (data, target) in enumerate(trainset_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if iteration % log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    ep, batch_idx * len(data), len(trainset_loader.dataset),\n",
    "                    100. * batch_idx / len(trainset_loader), loss.item()))\n",
    "            # different from before: saving model checkpoints\n",
    "            if iteration % save_interval == 0 and iteration > 0:\n",
    "                save_checkpoint('mnist-%i.pth' % iteration, model, optimizer)\n",
    "            iteration += 1\n",
    "        test()\n",
    "    \n",
    "    # save the final model\n",
    "    save_checkpoint('mnist-%i.pth' % iteration, model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_save(5, save_interval=500, log_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new model\n",
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "# load from the final checkpoint\n",
    "load_checkpoint('mnist-4690.pth', model, optimizer)\n",
    "# should give you the final model accuracy\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "### More about pytorch:\n",
    "* Using torch on GPU and multi-GPU - [link](http://pytorch.org/docs/master/notes/cuda.html)\n",
    "* More tutorials on pytorch - [link](http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n",
    "* Pytorch examples - a repo that implements many cool DL models in pytorch - [link](https://github.com/pytorch/examples)\n",
    "* Practical pytorch - a repo that implements some... other cool DL models... yes, in pytorch - [link](https://github.com/spro/practical-pytorch)\n",
    "* And some more - [link](https://www.reddit.com/r/pytorch/comments/6z0yeo/pytorch_and_pytorch_tricks_for_kaggle/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python37664bit9a436a4a37084a1abd1e3072c5d501f3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
